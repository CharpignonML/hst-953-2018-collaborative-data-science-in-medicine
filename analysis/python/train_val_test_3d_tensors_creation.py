# -*- coding: utf-8 -*-
"""Train_Val_Test_3D_Tensors_Creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNljHbgreJ1j5D-AzMmPEMt-37tccU7b
"""

# From Google Drive
from google.colab import drive
drive.mount('/content/drive')

# After executing cell above, Drive files will be presented on:
#'/content/drive/My Drive'
!ls "/content/drive/My Drive/"

## Imports all the packages. 
import numpy as np
import pandas as pd
import os 
from sklearn.cross_validation import train_test_split
import pdb

import random
from IPython.core.display import display, HTML

from google.colab import auth
from google.cloud import bigquery
from google.colab import files
import os

auth.authenticate_user() # authenticating

project_id='hst-953-2018'
os.environ["GOOGLE_CLOUD_PROJECT"]=project_id
# Read data from BigQuery into pandas dataframes.
def run_query(query):
  return pd.io.gbq.read_gbq(query, project_id=project_id, verbose=False, configuration={'query':{'useLegacySql': False}})

client = bigquery.Client()

"""### Test on a small dataset

### Time-varying features
"""

# Sample set for patients with subject_id in (3,4,17,20,21,23,25)
sample_X = run_query('''
select distinct *
from (
select t.subject_id,
t.hadm_id,
t.icustay_id,
t.hr,

-- Labs
ALB.ALBUMIN_avgval,
ALT.ALT_avgval,
AMYLASE.AMYLASE_avgval,
ANION_GAP.ANION_GAP_avgval,
AST.AST_avgval,
BANDS.BANDS_avgval,
BICARBONATE.BICARBONATE_avgval,
BILIRUBIN.BILIRUBIN_avgval,
BUN.BUN_avgval,
CHLORIDE.CHLORIDE_avgval,
CK.CK_avgval,
CKISO.CKISO_avgval,
CREATININE.CREATININE_avgval,
CRP.CRP_avgval,
D_DIMER.D_DIMER_avgval,
FIBRINOGEN.FIBRINOGEN_avgval,
FREE_CALCIUM.FREE_CALCIUM_avgval,
GLUCOSE.GLUCOSE_avgval,
HEMATOCRIT.HEMATOCRIT_avgval,
HEMOGLOBIN.HEMOGLOBIN_avgval,
INR.INR_avgval,
LACTATE.LACTATE_avgval,
LIPASE.LIPASE_avgval,
NEUTROPHILS.NEUTROPHILS_avgval,
NTPROBNP.NTPROBNP_avgval,
PH.PH_avgval,
PLATELET.PLATELET_avgval,
POTASSIUM.POTASSIUM_avgval,
--PT.PT_avgval,
PTT.PTT_avgval,
RBC.RBC_avgval,
SODIUM.SODIUM_avgval,
TROPONIN_T.TROPONIN_T_avgval,
WBC.WBC_avgval,

-- VITALS
HR.HR_avgval,
DiasBP.DiasBP_avgval,
SysBP.SysBP_avgval,
MeanBP.MeanBP_avgval,
RespRate.RespRate_avgval,
TempC.TempC_avgval,
CRYSTALLOID.CRYSTALLOID_binary,
CRYSTALLOID.CRYSTALLOID_sum,
VASOPRESSORS.VASOPRESSOR_binary
from `team_M.to_be_merged` t

left join `team_M.ALB_join`as ALB on t.subject_id = ALB.subject_id and t.hadm_id = ALB.hadm_id and t.icustay_id = ALB.icustay_id and t.hr = ALB.hr
left join `team_M.ALT_join`as ALT on t.subject_id = ALT.subject_id and t.hadm_id = ALT.hadm_id and t.icustay_id = ALT.icustay_id and t.hr = ALT.hr
left join `team_M.AMYLASE_join`as AMYLASE on t.subject_id = AMYLASE.subject_id and t.hadm_id = AMYLASE.hadm_id and t.icustay_id = AMYLASE.icustay_id and t.hr = AMYLASE.hr
left join `team_M.ANION_GAP_join`as ANION_GAP on t.subject_id = ANION_GAP.subject_id and t.hadm_id = ANION_GAP.hadm_id and t.icustay_id = ANION_GAP.icustay_id and t.hr = ANION_GAP.hr
left join `team_M.AST_join`as AST on t.subject_id = AST.subject_id and t.hadm_id = AST.hadm_id and t.icustay_id = AST.icustay_id and t.hr = AST.hr
left join `team_M.BANDS_join`as BANDS on t.subject_id = BANDS.subject_id and t.hadm_id = BANDS.hadm_id and t.icustay_id = BANDS.icustay_id and t.hr = BANDS.hr
left join `team_M.BICARBONATE_join`as BICARBONATE on t.subject_id = BICARBONATE.subject_id and t.hadm_id = BICARBONATE.hadm_id and t.icustay_id = BICARBONATE.icustay_id and t.hr = BICARBONATE.hr
left join `team_M.BILIRUBIN_join`as BILIRUBIN on t.subject_id = BILIRUBIN.subject_id and t.hadm_id = BILIRUBIN.hadm_id and t.icustay_id = BILIRUBIN.icustay_id and t.hr = BILIRUBIN.hr
left join `team_M.BUN_join`as BUN on t.subject_id = BUN.subject_id and t.hadm_id = BUN.hadm_id and t.icustay_id = BUN.icustay_id and t.hr = BUN.hr
left join `team_M.CHLORIDE_join`as CHLORIDE on t.subject_id = CHLORIDE.subject_id and t.hadm_id = CHLORIDE.hadm_id and t.icustay_id = CHLORIDE.icustay_id and t.hr = CHLORIDE.hr
left join `team_M.CK_join`as CK on t.subject_id = CK.subject_id and t.hadm_id = CK.hadm_id and t.icustay_id = CK.icustay_id and t.hr = CK.hr
left join `team_M.CKISO_join`as CKISO on t.subject_id = CKISO.subject_id and t.hadm_id = CKISO.hadm_id and t.icustay_id = CKISO.icustay_id and t.hr = CKISO.hr
left join `team_M.CREATININE_join`as CREATININE on t.subject_id = CREATININE.subject_id and t.hadm_id = CREATININE.hadm_id and t.icustay_id = CREATININE.icustay_id and t.hr = CREATININE.hr
left join `team_M.CRP_join`as CRP on t.subject_id = CRP.subject_id and t.hadm_id = CRP.hadm_id and t.icustay_id = CRP.icustay_id and t.hr = CRP.hr
left join `team_M.D_DIMER_join`as D_DIMER on t.subject_id = D_DIMER.subject_id and t.hadm_id = D_DIMER.hadm_id and t.icustay_id = D_DIMER.icustay_id and t.hr = D_DIMER.hr
left join `team_M.FIBRINOGEN_join`as FIBRINOGEN on t.subject_id = FIBRINOGEN.subject_id and t.hadm_id = FIBRINOGEN.hadm_id and t.icustay_id = FIBRINOGEN.icustay_id and t.hr = FIBRINOGEN.hr
left join `team_M.FREE_CALCIUM_join`as FREE_CALCIUM on t.subject_id = FREE_CALCIUM.subject_id and t.hadm_id = FREE_CALCIUM.hadm_id and t.icustay_id = FREE_CALCIUM.icustay_id and t.hr = FREE_CALCIUM.hr
left join `team_M.GLUCOSE_join` as GLUCOSE on t.subject_id = GLUCOSE.subject_id and t.hadm_id = GLUCOSE.hadm_id and t.icustay_id = GLUCOSE.icustay_id and t.hr = GLUCOSE.hr
left join `team_M.HEMATOCRIT_join`as HEMATOCRIT on t.subject_id = HEMATOCRIT.subject_id and t.hadm_id = HEMATOCRIT.hadm_id and t.icustay_id = HEMATOCRIT.icustay_id and t.hr = HEMATOCRIT.hr
left join `team_M.HEMOGLOBIN_join`as HEMOGLOBIN on t.subject_id = HEMOGLOBIN.subject_id and t.hadm_id = HEMOGLOBIN.hadm_id and t.icustay_id = HEMOGLOBIN.icustay_id and t.hr = HEMOGLOBIN.hr
left join `team_M.INR_join` as INR on t.subject_id = INR.subject_id and t.hadm_id = INR.hadm_id and t.icustay_id = INR.icustay_id and t.hr = INR.hr
left join `team_M.LACTATE_join`as LACTATE on t.subject_id = LACTATE.subject_id and t.hadm_id = LACTATE.hadm_id and t.icustay_id = LACTATE.icustay_id and t.hr = LACTATE.hr
left join `team_M.LIPASE_join`as LIPASE on t.subject_id = LIPASE.subject_id and t.hadm_id = LIPASE.hadm_id and t.icustay_id = LIPASE.icustay_id and t.hr = LIPASE.hr
left join `team_M.NEUTROPHILS_join`as NEUTROPHILS on t.subject_id = NEUTROPHILS.subject_id and t.hadm_id = NEUTROPHILS.hadm_id and t.icustay_id = NEUTROPHILS.icustay_id and t.hr = NEUTROPHILS.hr
left join `team_M.NTPROBNP_join`as NTPROBNP on t.subject_id = NTPROBNP.subject_id and t.hadm_id = NTPROBNP.hadm_id and t.icustay_id = NTPROBNP.icustay_id and t.hr = NTPROBNP.hr
left join `team_M.PH_join` as PH on t.subject_id = PH.subject_id and t.hadm_id = PH.hadm_id and t.icustay_id = PH.icustay_id and t.hr = PH.hr
left join `team_M.PLATELET_join` as PLATELET on t.subject_id = PLATELET.subject_id and t.hadm_id = PLATELET.hadm_id and t.icustay_id = PLATELET.icustay_id and t.hr = PLATELET.hr
left join `team_M.POTASSIUM_join` as POTASSIUM on t.subject_id = POTASSIUM.subject_id and t.hadm_id = POTASSIUM.hadm_id and t.icustay_id = POTASSIUM.icustay_id and t.hr = POTASSIUM.hr
--left join `team_M.PT_join`as PT on t.subject_id = PT.subject_id and t.hadm_id = PT.hadm_id and t.icustay_id = PT.icustay_id and t.hr = PT.hr
left join `team_M.PTT_join`as PTT on t.subject_id = PTT.subject_id and t.hadm_id = PTT.hadm_id and t.icustay_id = PTT.icustay_id and t.hr = PTT.hr
left join `team_M.RBC_join`as RBC on t.subject_id = RBC.subject_id and t.hadm_id = RBC.hadm_id and t.icustay_id = RBC.icustay_id and t.hr = RBC.hr
left join `team_M.SODIUM_join`as SODIUM on t.subject_id = SODIUM.subject_id and t.hadm_id = SODIUM.hadm_id and t.icustay_id = SODIUM.icustay_id and t.hr = SODIUM.hr
left join `team_M.TROPONIN_T_join`as TROPONIN_T on t.subject_id = TROPONIN_T.subject_id and t.hadm_id = TROPONIN_T.hadm_id and t.icustay_id = TROPONIN_T.icustay_id and t.hr = TROPONIN_T.hr
left join `team_M.WBC_join`as WBC on t.subject_id = WBC.subject_id and t.hadm_id = WBC.hadm_id and t.icustay_id = WBC.icustay_id and t.hr = WBC.hr

-- Vital signs
left join `team_M.HR_join`as HR on t.subject_id = HR.subject_id and t.hadm_id = HR.hadm_id and t.icustay_id = HR.icustay_id and t.hr = HR.hr
left join `team_M.DiasBP_join`as DiasBP on t.subject_id = DiasBP.subject_id and t.hadm_id = DiasBP.hadm_id and t.icustay_id = DiasBP.icustay_id and t.hr = DiasBP.hr
left join `team_M.SysBP_join`as SysBP on t.subject_id = SysBP.subject_id and t.hadm_id = SysBP.hadm_id and t.icustay_id = SysBP.icustay_id and t.hr = SysBP.hr
left join `team_M.MeanBP_join`as MeanBP on t.subject_id = MeanBP.subject_id and t.hadm_id = MeanBP.hadm_id and t.icustay_id = MeanBP.icustay_id and t.hr = MeanBP.hr
left join `team_M.RespRate_join`as RespRate on t.subject_id = RespRate.subject_id and t.hadm_id = RespRate.hadm_id and t.icustay_id = RespRate.icustay_id and t.hr = RespRate.hr
left join `team_M.TempC_join`as TempC on t.subject_id = TempC.subject_id and t.hadm_id = TempC.hadm_id and t.icustay_id = TempC.icustay_id and t.hr = TempC.hr

-- vasopressors
left join `team_M.VASOPRESSORS_join_reviewed_new_2`as VASOPRESSORS on t.subject_id = VASOPRESSORS.subject_id and t.hadm_id = VASOPRESSORS.hadm_id and t.icustay_id = VASOPRESSORS.icustay_id and t.hr = VASOPRESSORS.hr

-- crystalloids
left join `team_M.CRYSTALLOID_join_reviewed_new_2`as CRYSTALLOID on t.subject_id = CRYSTALLOID.subject_id and t.hadm_id = CRYSTALLOID.hadm_id and t.icustay_id = CRYSTALLOID.icustay_id and t.hr = CRYSTALLOID.hr

--WHERE t.subject_id in (3,4,17,20,21,23,25)
WHERE t.subject_id < 1000

ORDER BY subject_id, hadm_id, icustay_id, hr)
ORDER BY subject_id, hadm_id, icustay_id, hr
''')

# Convert to a data frame
sample_X = pd.DataFrame(sample_X)

# Sample Y
sample_Y = run_query('''
select t.*,
if(y.PF_minval_less is null, 0, y.PF_minval_less) as Y
from  `team_M.to_be_merged` t
left join  `team_M.PaO2_FiO2_ratios_per_hour_with_booleans` y
on t.subject_id = y.subject_id and t.hadm_id = y.hadm_id and t.icustay_id = y.icustay_id and t.hr = y.hr
--where t.subject_id in (3,4,17,20,21,23,25)
where t.subject_id < 1000''')

# Convert to a data frame
sample_Y = pd.DataFrame(sample_Y)

# Y at the icustay id level for stratification
sample_Y_for_stratification = run_query('''
select t.subject_id,
t.hadm_id,
t.icustay_id,
if(y.max_PF is null, 0, y.max_PF) as Y
from `team_M.to_be_merged` t 
left join (select subject_id,
hadm_id,
icustay_id,
max(PF_minval_less) as max_PF
from `team_M.PaO2_FiO2_ratios_per_hour_with_booleans`
group by subject_id, hadm_id, icustay_id
order by subject_id, hadm_id, icustay_id) y
on t.subject_id = y.subject_id and t.hadm_id = y.hadm_id and t.icustay_id = y.icustay_id
--where t.subject_id in (3,4,17,20,21,23,25)
where t.subject_id < 1000
group by subject_id, hadm_id, icustay_id, max_PF
order by subject_id, hadm_id, icustay_id''')

# Convert to a data frame
sample_Y_for_stratification = pd.DataFrame(sample_Y_for_stratification)

# Confirm dimensions of sample_X and sample_Y are identical

# sample_X
print("Shape of sample_X: ", sample_X.shape, "\n")
# Only if sample_X is small, print it:
if sample_X.shape[0] < 100:
  print("List of unique subject ids in sample_X: ", np.unique(sample_X['subject_id']), "\n")
print("sample_X vars: ", sample_X.columns, "\n")


# sample_Y
print("Shape of sample_Y: ", sample_Y.shape, "\n")
# Only if sample_Y is small, print it:
if sample_Y.shape[0] < 100:
  print("List of unique subject ids in sample_Y: ", np.unique(sample_Y['subject_id']), "\n")
print("sample_Y vars: ", sample_Y.columns, "\n")

# sample_Y_for_stratification
print("Shape of sample_Y_for_stratification: ", sample_Y_for_stratification.shape, "\n")
# Only if sample_Y_for_stratification is small, print it:
if sample_Y_for_stratification.shape[0] < 100:
  print("List of unique subject ids in sample_Y_for_stratification: ", np.unique(sample_Y_for_stratification['subject_id']), "\n")
print("sample_Y_for_stratification vars: ", sample_Y_for_stratification.columns)

"""### Static datasets"""

static = run_query('''
select subject_id,
hadm_id,
icustay_id,
intime,
outtime,
hospital_expire_flag,
gender,
age_category,
ethnicity_category,
--height_filled,
--weight_filled,
--BMI,
admission_type,
admission_location_category,
insurance_category,
marital_status_category
from `team_M.demographics2_final_new`
where subject_id < 1000
order by subject_id, hadm_id,icustay_id''')

# Convert to a data frame
static = pd.DataFrame(static)

lengths = run_query('''
select distinct subject_id, 
         hadm_id,
         icustay_id,
         if(floor(los*24)+1 > 168, 168, floor(los*24)+1)  as los 
 from `team_M.LuStep1_new_new`
 where icu_stay_rank = 1 and subject_id < 1000
 -- and subject_id in (3,4,17,20,21,23,25)
 group by subject_id, hadm_id, icustay_id, los
 order by subject_id, hadm_id, icustay_id;''')

# Convert to a data frame
lengths_df = pd.DataFrame(lengths)

# Adding index column (just in case)
lengths_add = pd.DataFrame(list(range(0,lengths.shape[0])))
lengths_df[['index']] = lengths_add

# Only if sample_X is small, print it:
if sample_X.shape[0] < 100:
  print(lengths_df)

# Just checking that the order in which icustay ids are stored is the same for
# sample_X and lengths, when patient is identical.
print(lengths_df.loc[lengths_df['subject_id']==23])
print(sample_X.loc[sample_X['subject_id']==23])

# Recap - Variable Names

print("X column names: ", sample_X.columns.values, "\n")
print("Y column names: ",sample_Y.columns.values, "\n")
print("STATIC column names: ", static.columns.values, "\n")
print("LENGTHS column names: ", lengths_df.columns.values)

print("X data types: ", sample_X.dtypes, "\n")
# crystalloid_binary and vasopressor_binary are considered as objects
# we need to change them to float
sample_X = sample_X.astype('float64')
# Confirm that it got modified
print("X new data types: ", sample_X.dtypes, "\n")
# Check that the shape has stayed the same
print("X sample size: ", sample_X.shape)

"""### Preprocessing"""

# Inspired from load_data.py in Harini's code

def load_data(X, Y, static): 
    print("Shape of X at the beginning: ", X.shape)
    print("Shape of Y at the beginning: ", Y.shape)
    print("Shape of static at the beginning: ", static.shape)
    ## just getting the unique subject ids, not repeats (since multiple admissions are possible)
    static = static[static.subject_id.isin(np.unique(Y.subject_id))]
    ## only getting these variables for the Y dataframe
    Y = Y[['subject_id','hadm_id','icustay_id','hr','Y']]# What about this indexing?

    # Map the lowering function to all column names
    X.columns = map(str.lower, X.columns)
    ## first four variables are just subject_id, hadm_id, icustay_id, hr
    var_names  = list(X.columns.values[4:])
    Y.columns = map(str.lower, Y.columns)
    out_names  = list(Y.columns.values[4:])
    static.columns = map(str.lower, static.columns)
    static_names = list(static.columns.values[4:])

    print('Shape of X: ', X.shape)
    print('Shape of Y: ', Y.shape)
    print('Shape of static: ', static.shape)
    print('Variable names: ', ",".join(var_names))
    print('Output names: ', ",".join(out_names))
    print('Static data: ', ",".join(static_names))

    return X, Y, static

# Load datasets
load_data(sample_X, sample_Y, static)

# Check again all the column names for sample_X, sample_Y and static
print("sample_X new column names: ", sample_X.columns, "\n")
print("sample_Y new column names: ", sample_Y.columns, "\n")
print("static new column names: ", static.columns)

# Inspired from preprocessing.py in Harini's code

from sklearn.cross_validation import train_test_split
import pandas as pd
import numpy as np
import os
import pdb

# 7 days times 24 hours + 1 (because we start at hour 0)
max_len = 7*24 + 1

# Needed for the clip
indices = [-4.0,-3.0,-2.0,-1.0,0.0,1.0,2.0,3.0,4.0,9.0]
def categorization(l):
  sub_l = []
  for ll in l:
    # for debug: print(ll)
    # for debug: print(ll+indices[0])
    sub_ll = [str(ll)+str(indice) for indice in indices]
    sub_l = sub_l + sub_ll
  
  return(sub_l)

# Gets the rounded z-score, all should have same number of categories (-4 to 4, including 0)
# Fill NAs with 9s and cut this column out later
def transform_vals(x, normal_dict, std_dict):
    x = 1.0*(x - normal_dict[x.name])/std_dict[x.name]
    x = x.round()
    x = x.clip(-4,4)
    x = x.fillna(9)
    
    return x

# Compute mean and std, get z-score for each feature, turns everything into z-scores
# For Harini's work, that is the option that performed the best.
def make_discrete_values(mat):
    print(mat.shape)
    mat_without_bins = mat.drop(['vasopressor_binary','crystalloid_binary'],axis=1)
    print(mat_without_bins.shape)
    
    normal_dict = mat_without_bins.groupby(['subject_id']).mean().mean().to_dict()
    print(normal_dict)
    std_dict = mat_without_bins.std().to_dict()
    print(std_dict)
    
    # start at column 4 
    feature_cols = mat_without_bins.columns[4:]
    print(feature_cols)
    X_words = mat_without_bins.loc[:,feature_cols]
    
    print(X_words.shape)
    X_words_1 = X_words.apply(lambda x: transform_vals(x, normal_dict, std_dict), axis=0)
    print(X_words_1)
    print(X_words_1.shape)
    mat_without_bins.loc[:,feature_cols] = X_words_1
    cats = categorization(feature_cols)
    print(cats)
    X_categorized = pd.get_dummies(mat_without_bins, columns = mat_without_bins.columns[4:])
    X_categorized = X_categorized.reindex(columns=cats, fill_value=0)
    print(X_categorized.shape)
    na_columns = [col for col in X_categorized.columns if '_9' in col]
    print(na_columns)
    X_categorized.drop(na_columns, axis=1, inplace=True)
    X_categorized_1 = X_categorized.merge(mat[['subject_id','hadm_id','icustay_id','hr','CRYSTALLOID_binary','VASOPRESSOR_binary']],on=['subject_id','hadm_id','icustay_id','hr'],how="left")
   
    return X_categorized_1

# At the moment, average value for each feature variable as well as min and max are hard-coded to make things easier.
# This will be modified in the future.

# Averages
avg_dict = {'hr': 84.0, 'albumin_avgval': 2.91059768084959, 'alt_avgval':265.393028645872, 
            'amylase_avgval':118.652502463978 , 'anion_gap_avgval':13.5459997478002 , 'ast_avgval': 310.161991149104, 
            'bands_avgval':7.9365864190867, 'bicarbonate_avgval':24.9829427646123, 'bilirubin_avgval':3.02092870080686, 
            'bun_avgval':27.6848933222158, 'chloride_avgval': 104.572738812181, 'ck_avgval':1403.30476447231, 
            'ckiso_avgval': 27.5466184222695, 'creatinine_avgval': 1.50735008238658, 'crp_avgval': 82.5622744220731, 
            'd_dimer_avgval': 4121.40505824647, 'fibrinogen_avgval': 311.469855162942, 'free_calcium_avgval':1.131272622194 ,
            'glucose_avgval': 133.15599790227, 'hematocrit_avgval': 30.5217959049053, 'hemoglobin_avgval': 10.3847297253617, 'inr_avgval':1.57739149783264,
            'lactate_avgval': 2.65744758133579, 'lipase_avgval': 184.352406293383, 'neutrophils_avgval':76.5885835709737 , 
            'ntprobnp_avgval': 9117.69802317653, 'ph_avgval': 7.22521493555536, 'platelet_avgval': 207.829541011297, 
            'potassium_avgval':4.09437343395222 , 'ptt_avgval':43.5301572399898 , 
            'rbc_avgval': 3.48280983306403, 'sodium_avgval': 138.679091035021, 'troponin_t_avgval': 0.907275485828658, 
            'wbc_avgval': 11.2297838812557, 'hr_avgval': 85.3661840972615, 'diasbp_avgval': 62.2055720726778, 
            'sysbp_avgval': 120.762843903791, 'meanbp_avgval': 78.3739451617918, 'resprate_avgval':19.5244757414734 , 
            'tempc_avgval': 36.8926962004831, 'crystalloid_binary': 0.0, 'crystalloid_sum': 7.75739319560433, 'vasopressor_binary': 0.0}

# Minima
mins_df = pd.DataFrame([[0.0,1.0 ,1.0 ,2.0 , 1.0, 1.0, 0.8,5 ,0.1 ,1.0 ,3.4 ,3.0 ,0.9 , 0.05, 0.06, 95.0, 31.0, 0.092,1.55,2.1,1.6,0.1,0.05,1,0.3,10,0.94,5,0.8,0.15,0.000001,3.8,0.01,0.1,4,1,0.2,0.43,1,25,0.0,0,0.0]],
                       columns = ['hr','albumin_avgval','alt_avgval','amylase_avgval','anion_gap_avgval','ast_avgval','bands_avgval','bicarbonate_avgval','bilirubin_avgval','bun_avgval',
                                  'chloride_avgval','ck_avgval','ckiso_avgval','creatinine_avgval','crp_avgval','d_dimer_avgval','fibrinogen_avgval','free_calcium_avgval','glucose_avgval',
                                  'hematocrit_avgval','hemoglobin_avgval','inr_avgval','lactate_avgval','lipase_avgval','neutrophils_avgval','ntprobnp_avgval','ph_avgval','platelet_avgval',
                                  'potassium_avgval', 'ptt_avgval','rbc_avgval','sodium_avgval','troponin_t_avgval','wbc_avgval','hr_avgval','diasbp_avgval','sysbp_avgval','meanbp_avgval','resprate_avgval','tempc_avgval','crystalloid_binary','crystalloid_sum','vasopressor_binary'] )

mins_series = mins_df.iloc[0]

# Maxima
maxes_df = pd.DataFrame([[168.0, 6.9, 25460.0, 22020.0, 77.0, 36400.0, 79.0, 55.0, 82.2, 270.0, 198.0, 266720.0, 3320.0, 138.0, 299.9, 36650.0, 1773.0, 97.0, 3200.0, 77.7, 22.1, 48.7, 32.0, 1210000.0,100.0,118928.0,10.0,2813.0, 27.5,150.0,1300.0,183.0,29.91,665.6,280.0,294.0,329.0,299.0,69.0,42.0,1.0,11000.0,1.0]],
                       columns = ['hr','albumin_avgval','alt_avgval','amylase_avgval','anion_gap_avgval','ast_avgval','bands_avgval','bicarbonate_avgval','bilirubin_avgval','bun_avgval',
                                  'chloride_avgval','ck_avgval','ckiso_avgval','creatinine_avgval','crp_avgval','d_dimer_avgval','fibrinogen_avgval','free_calcium_avgval','glucose_avgval',
                                  'hematocrit_avgval','hemoglobin_avgval','inr_avgval','lactate_avgval','lipase_avgval','neutrophils_avgval','ntprobnp_avgval','ph_avgval','platelet_avgval',
                                  'potassium_avgval', 'ptt_avgval','rbc_avgval','sodium_avgval','troponin_t_avgval','wbc_avgval','hr_avgval','diasbp_avgval','sysbp_avgval','meanbp_avgval','resprate_avgval','tempc_avgval','crystalloid_binary','crystalloid_sum','vasopressor_binary'] )

maxes_series = maxes_df.iloc[0]

# Doesn't actually backfill, but normalizes instead.
def backfill_normalize(X, minima, maxima, averages):
    X_index = X.set_index(['subject_id','hadm_id', 'icustay_id'])
    # First forward fill existing values, at the ICU stay level
    filled = X_index.groupby(level=0).fillna(method='ffill')
    #filled_index = filled.set_index(['subject_id','hadm_id', 'icustay_id'])
    #filled_index2 = filled_index.groupby(level=0).fillna(method='backfill')
    
    # Make dictionary of population mean values for all patients
    print("Calculating the means...\n")
    #normal_dict = X_index.mean(level=0).mean().to_dict()
    normal_dict = averages
    print(normal_dict)
    print(type(normal_dict))
    # Fill in remaining NaNs with mean values, {feat:mean}
    print("Filling NAs...\n")
    X_clean = filled.fillna(normal_dict)
    
    # normalize
    #mins = X_index.min()
    mins = minima
    print(type(mins))
    
    # Just in case everyone is at 0 or 1 in the sample (which can happen with small datasets), we need to set the min at 0
    #mins['vasopressor_binary'] = 0
    #mins['crystalloid_binary'] = 0
    
    #maxes = X_index.max()
    maxes = maxima
    print(type(maxes))
    
    # Just in case everyone is at 0 or 1 in the sample (which can happen with small datasets), we need to set the max at 1
    #maxes['vasopressor_binary'] = 1
    #maxes['crystalloid_binary'] = 1
    
    # Quick checks
    print("Min for each variable: ", mins, "\n")
    print("Max for each variable: ", maxes, "\n")
    
    print("Normalizing...\n")
    normalized = (X_clean - mins) / (maxes - mins)
    normalized = normalized.reset_index()
    
    # The "hr" column needs to be reset to normal
    normalized['hr'] = normalized['hr']*(max_len-1)
    
    # Quick final check
    print(normalized.head())
    
    return normalized


def get_filled_dataframe(X, minima, maxima, averages, make_discrete):
	if make_discrete:
		print (' Making Discrete Dataframe ... ')
		X_processed = pd.DataFrame(X)
		X_processed = make_discrete_values(X_processed)

	else: 
		print('Making backfilled / normalized Dataframe')
		X_processed = backfill_normalize(X,minima,maxima,averages)
	
  # Checking which columns of the dataset have been processed
	print("Column names that have been processed: ", X_processed.columns, "\n")
	return X_processed

def make_static_matrix(static):
  
  # Let us not include weight, height and BMI because more than half is missing
  static_to_keep = static[['subject_id', 'hadm_id', 'icustay_id', 'intime', 'gender', 'age_category' ,'ethnicity_category','admission_type','admission_location_category','marital_status_category']]
  static_to_keep.loc[:, 'intime'] = static_to_keep['intime'].astype('datetime64').apply(lambda x : x.hour)
  static_to_keep.loc[:, 'subject_id'] = static_to_keep['subject_id'].astype(int)
  static_to_keep = pd.get_dummies(static_to_keep, columns = ['gender','admission_type','admission_location_category',
                                                            'marital_status_category','ethnicity_category' ])
  
  current_cols = static_to_keep.columns.tolist()
  # For debug: print(current_cols)
  
  # Even for reasonably large samples, it can happen that one of the categories is not represented.
  # In that case, we have to manually create the dummy variable.
  
  # Check "admission type" feature
  
  # Check "elective" category
  check_presence_elec = 'admission_type_ELECTIVE' in current_cols
  if check_presence_elec == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('gender_M')
    static_to_keep.insert(pos+1,'admission_type_ELECTIVE', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "emergency" category
  check_presence_em = 'admission_type_EMERGENCY' in current_cols
  if check_presence_em == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_type_ELECTIVE')
    static_to_keep.insert(pos+1,'admission_type_EMERGENCY', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "urgent" category
  check_presence_urg = 'admission_type_URGENT' in current_cols
  if check_presence_urg == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_type_EMERGENCY')
    static_to_keep.insert(pos+1,'admission_type_URGENT', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "admission location" feature
     
  # Check "clinic referral" category
  check_presence_ref = 'admission_location_category_CLINIC_REFERRAL' in current_cols
  if check_presence_ref == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_type_URGENT')
    static_to_keep.insert(pos+1,'admission_location_category_CLINIC_REFERRAL', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "emergency room" category
  check_presence_emroom = 'admission_location_category_EMERGENCY_ROOM' in current_cols
  if check_presence_emroom == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_location_category_CLINIC_REFERRAL')
    static_to_keep.insert(pos+1,'admission_location_category_EMERGENCY_ROOM', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "hosp transfer" category
  check_presence_hosptrans = 'admission_location_category_HOSP_TRANSFER' in current_cols
  if check_presence_hosptrans == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_location_category_EMERGENCY_ROOM')
    static_to_keep.insert(pos+1,'admission_location_category_HOSP_TRANSFER', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "other" category
  check_presence_other = 'admission_location_category_OTHER' in current_cols
  if check_presence_other == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_location_category_HOSP_TRANSFER')
    static_to_keep.insert(pos+1,'admission_location_category_OTHER', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "phys referral" category
  check_presence_physref = 'admission_location_category_PHYS_REFERRAL' in current_cols
  if check_presence_physref == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_location_category_OTHER')
    static_to_keep.insert(pos+1,'admission_location_category_PHYS_REFERRAL', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
 
  # Check "marital status" feature
  
  # Check "divorced" category
  check_presence_divorced = 'marital_status_category_DIVORCED' in current_cols
  if check_presence_divorced == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('admission_location_category_PHYS_REFERRAL')
    static_to_keep.insert(pos+1,'marital_status_category_DIVORCED', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
    
  # Check "married" category
  check_presence_married = 'marital_status_category_MARRIED' in current_cols
  if check_presence_married == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('marital_status_category_DIVORCED')
    static_to_keep.insert(pos+1,'marital_status_category_MARRIED', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check "other" category
  check_presence_other = 'marital_status_category_OTHER' in current_cols
  if check_presence_other == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('marital_status_category_MARRIED')
    static_to_keep.insert(pos+1,'marital_status_category_OTHER', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check "separated" category
  check_presence_sep = 'marital_status_category_SEPARATED' in current_cols
  if check_presence_sep == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('marital_status_category_OTHER')
    static_to_keep.insert(pos+1,'marital_status_category_SEPARATED', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check "single" category
  check_presence_single = 'marital_status_category_SINGLE' in current_cols
  if check_presence_single == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('marital_status_category_SEPARATED')
    static_to_keep.insert(pos+1,'marital_status_category_SINGLE', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check "widowed" category
  check_presence_widow = 'marital_status_category_WIDOWED' in current_cols
  if check_presence_widow == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('marital_status_category_SINGLE')
    static_to_keep.insert(pos+1,'marital_status_category_WIDOWED', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check "ethnicity category" feature
  
  # Check american indian
  check_presence_amid = 'ethnicity_category_AMERICAN INDIAN' in current_cols
  #print(check_presence)
  if check_presence_amid == False:
    #print("hello")
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    # pos 64
    pos = current_cols.index('marital_status_category_WIDOWED')
    print(pos)
    static_to_keep.insert(pos+1, 'ethnicity_category_AMERICAN INDIAN', static_to_keep_add) 
    current_cols = static_to_keep.columns.tolist()
    #print(static_to_keep.head())
    
  # Check asian
  check_presence_asian = 'ethnicity_category_ASIAN' in current_cols
  if check_presence_asian == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('ethnicity_category_AMERICAN INDIAN')
    static_to_keep.insert(pos+1,'ethnicity_category_ASIAN', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check black
  check_presence_black = 'ethnicity_category_BLACK' in current_cols
  if check_presence_black == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('ethnicity_category_ASIAN')
    static_to_keep.insert(pos+1,'ethnicity_category_BLACK', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check hispanic latino
  check_presence_hisp = 'ethnicity_category_HISPANIC/LATINO' in current_cols
  if check_presence_hisp == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('ethnicity_category_BLACK')
    static_to_keep.insert(pos+1,'ethnicity_category_HISPANIC/LATINO', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check other
  check_presence_other = 'ethnicity_category_OTHER' in current_cols
  if check_presence_other == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('ethnicity_category_HISPANIC/LATINO')
    static_to_keep.insert(pos+1,'ethnicity_category_OTHER', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  # Check white
  check_presence_white = 'ethnicity_category_WHITE' in current_cols
  if check_presence_white == False:
    static_to_keep_add = pd.DataFrame(np.zeros(static_to_keep.shape[0]))
    pos = current_cols.index('ethnicity_category_OTHER')
    static_to_keep.insert(pos+1,'ethnicity_category_WHITE', static_to_keep_add)
    current_cols = static_to_keep.columns.tolist()
  
  print("List of static columns with NAs: ", static_to_keep.columns[static_to_keep.isna().any()].tolist(), "\n")
  
  return static_to_keep

# Converts intime into datetime64
def get_intime(static):
	intime = static[['subject_id', 'hadm_id', 'icustay_id','intime']]#.dropna()
	intime['intime'] = intime['intime'].astype('datetime64').apply(lambda x : x.hour)
	return intime

# Two ways of normalizing the data
# make_discrete --> z-score
# Option: add_abs_time --> exact time of day will get added
# Option: add_static --> replicates columns with demographic information
def process_data(X, Y, minima, maxima, averages, static, make_discrete=False, add_static=True, add_abs_time=True):
  X_copy = X
  #print("after X copy: ", X_copy.shape)
  print("Start of process data\n")
  print("Columns names getting processed: ", X_copy.columns, "\n")
  X_copy = get_filled_dataframe(X_copy, minima, maxima, averages, make_discrete)
  #print("after fill: ", X_copy.shape)
  
  num_static_cols = 0
  
  if add_static:
    print("Now processing static...\n")
    static_to_keep = make_static_matrix(static)

    X_copy = pd.merge(X_copy, static_to_keep, on=['subject_id','hadm_id', 'icustay_id'], how="left")
    #print("after merge 1: ", X_copy.shape)
        # explicitly adds the hours in as a feature, actual time of day 
    if add_abs_time: 
      abs_time = (X_copy['intime'] + X_copy['hr'])%24
      X_copy.insert(4, 'absolute_time', abs_time)
      #print("after insert: ", X_copy.shape)

    X_copy.drop('intime', axis=1, inplace=True)
    num_static_cols = static_to_keep.shape[1] - 2

  elif add_abs_time:
    print("Now adding absolute time column...\n")
    intime = get_intime(static)
    X_copy = pd.merge(X_copy, intime, on=['subject_id','hadm_id','icustay_id'], how="left")
    abs_time = (X_copy['intime'] + X_copy['hr'])%24
    X_copy.insert(4, 'absolute_time', abs_time)
    X_copy.drop('intime', axis=1, inplace=True)

  print("End of processing\n")
  return X_copy, num_static_cols
    
def make_3d_tensors(X, Y, lengths, shift_amount=0):
  X.set_index(['subject_id','hadm_id', 'icustay_id'])
  Y.set_index(['subject_id','hadm_id', 'icustay_id'])
  lengths.set_index(['subject_id','hadm_id', 'icustay_id'])
  
  print(X.shape)
  print(X.columns)
  
  X_tensor= np.array(X)
  print("X tensor shape: ", X_tensor.shape)
  X_tensor=X_tensor.reshape(X.shape[0]//max_len,max_len,X.shape[1])

  Y_tensor = np.array(Y['Y'])
  lengths_tensor = np.array(lengths['los'])
  print("Y_tensor shape: ", Y_tensor.shape)
  print("lengths tensor shape: ", lengths_tensor.shape)
  Y_tensor=Y_tensor.reshape(Y.shape[0]//max_len,max_len,1)
  lengths_tensor=lengths_tensor.reshape(lengths.shape[0],1)
  
  print(X_tensor.shape)
  print(Y_tensor.shape)
  print(lengths_tensor.shape)

  return X_tensor, Y_tensor, lengths_tensor

# Option to provide an id list or not
def stratified_split(X, Y, Y_for_stratification, static, lengths, train_id_list=None, test_id_list=None, random=0):
  train_ids, test_ids = train_test_split(Y_for_stratification, test_size=0.2, stratify=Y_for_stratification['Y'], random_state=random)
  train_ids, val_ids = train_test_split(train_ids, test_size = .125, stratify=train_ids['Y'], random_state=random)
  #print("list train ids", train_ids)
  train_ids_set = set()
  for i, row in train_ids.iterrows():
    train_ids_set.add((row[0], row[1], row[2]))
  #print("train ids set", train_ids_set)
  print("Done with training ids\n")

  val_ids_set = set()
  for i, row in val_ids.iterrows():
    val_ids_set.add((row[0], row[1], row[2]))
  print("Done with validation ids\n")

  test_ids_set = set()
  for i, row in test_ids.iterrows():
    test_ids_set.add((row[0], row[1], row[2]))
  print("Done with test ids\n")
    
  X.set_index(['subject_id', 'hadm_id', 'icustay_id'])
  Y.set_index(['subject_id', 'hadm_id', 'icustay_id'])
  X_grouped = X.groupby(['subject_id', 'hadm_id', 'icustay_id'])
  #print("groupby X: ",list(X_grouped.groups))
  Y_grouped = Y.groupby(['subject_id', 'hadm_id', 'icustay_id'])
  #print("groupby Y: ",list(Y_grouped.groups))
  lengths_grouped = lengths.groupby(['subject_id', 'hadm_id', 'icustay_id'])
  
  X_train = pd.concat([X_grouped.get_group(tid) for tid in train_ids_set])
  X_train_static = pd.concat([static.groupby(['subject_id', 'hadm_id', 'icustay_id']).get_group(tid) for tid in train_ids_set])
  
  Y_train = pd.concat([Y_grouped.get_group(tid) for tid in train_ids_set])
  lengths_train = pd.concat([lengths_grouped.get_group(tid) for tid in train_ids_set])

  X_val = pd.concat([X_grouped.get_group(vid) for vid in val_ids_set])
  X_val_static = pd.concat([static.groupby(['subject_id', 'hadm_id', 'icustay_id']).get_group(vid) for vid in val_ids_set])
  
  Y_val = pd.concat([Y_grouped.get_group(vid) for vid in val_ids_set])
  lengths_val = pd.concat([lengths_grouped.get_group(vid) for vid in val_ids_set])

  X_test = pd.concat([X_grouped.get_group(teid) for teid in test_ids_set])
  X_test_static = pd.concat([static.groupby(['subject_id', 'hadm_id', 'icustay_id']).get_group(teid) for teid in test_ids_set])
  Y_test = pd.concat([Y_grouped.get_group(teid) for teid in test_ids_set])
  lengths_test = pd.concat([lengths_grouped.get_group(teid) for teid in test_ids_set])
    

  return X_train, X_val, X_test, Y_train, Y_val, Y_test, lengths_train, lengths_val, lengths_test

# Inspired from preprocessing_for_sliding_window.py in Harini's work

from sklearn.cross_validation import train_test_split
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
import os
import pdb
import os.path 

def make_3d_tensor_slices(X_tensor, Y_tensor, lengths, slice_size, gap_time, outcome_type='binary', return_gap=False):

    num_patients = X_tensor.shape[0]
    print("Total number of ICU stay ids: ", num_patients)
    timesteps = X_tensor.shape[1]
    print(timesteps)
    print("Total number of time steps: ", timesteps)
    num_features = X_tensor.shape[2]
    print("Total number of features: ", num_features, "\n")

    # Y_tensor = Y_tensor[:, :, intervention_index]
    # In our case, the outcome is a 2D tensor because we are only looking at one type of outcome and the outcome itself is binary

    # New tensors should have the following shape: (# ICU stay ids, slice size = 6 hours by default, # features)
    X_tensor_new = np.zeros((num_patients*timesteps, slice_size, num_features))
    Y_tensor_new = np.zeros((num_patients*timesteps))
    gap_tensor = np.zeros((num_patients*timesteps, gap_time, num_features))

    # Prediction window set to 4 hours by default
    prediction_window = int(4)

    current_row = 0
    
    # We want to keep track of the aggregated length
    #total_length = 0
    
    #print("Starting to loop on ICU stay...")

    for patient_index in range(num_patients):
        if patient_index % 100 == 0: print(patient_index)
        x_patient = X_tensor[patient_index]
        y_patient = Y_tensor[patient_index]
        length = lengths[patient_index]
        length = int(length)
        #total_length += length
        
        #print("Patient index: ", patient_index)
        #print("Length: ",length)
        #print("timestep range :", length - prediction_window - gap_time - slice_size)

        #print("Starting to loop on timestep...")
       
        for timestep in range(length - prediction_window - gap_time - slice_size):
            x_window = x_patient[timestep:timestep+slice_size]
            y_window = y_patient[timestep:timestep+slice_size]
            result_window = y_patient[timestep+slice_size+gap_time:timestep+slice_size+gap_time+prediction_window]
            gap_window = y_patient[(timestep+slice_size):(timestep+slice_size+gap_time)]
            x_gap = x_patient[(timestep+slice_size):(timestep+slice_size+gap_time)]
            y_gap = y_patient[(timestep+slice_size):(timestep+slice_size+gap_time)]

            #print result_window, result_window_diff

            if outcome_type == 'binary':
                if gap_window.any() == 1:
                    result = None
                elif result_window.any() == 1:
                    #print("1")
                    result = 1
                elif result_window.all() == 0:
                    result = 0
                if result != None:
                    X_tensor_new[current_row] = x_window
                    Y_tensor_new[current_row] = result
                    gap_tensor[current_row] = x_gap
                    current_row += 1

    #print(X_tensor_new.shape)
    #print(Y_tensor_new.shape)
    #print(gap_tensor.shape)
    
    #print("Total length is equal to: ", total_length, "\n")
    X_tensor_new = X_tensor_new[:current_row,:,:]
    Y_tensor_new = Y_tensor_new[:current_row]
    gap_tensor = gap_tensor[:current_row,:,:]

    if return_gap:
        #print(X_tensor_new.shape)
        #print(Y_tensor_new.shape)
        #print(gap_tensor.shape)
        return X_tensor_new, Y_tensor_new, gap_tensor
      
    else: 
        #print(X_tensor_new.shape)
        #print(Y_tensor_new.shape)
        #print(gap_tensor.shape)
        return X_tensor_new, Y_tensor_new

def split_tensors(x_tensor, y_tensor):
    x_train_val, x_test, y_train_val, y_test =  train_test_split(x_tensor, y_tensor, test_size = 0.2, stratify = y_tensor)
    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size = .125, stratify = y_train_val)

    return x_train, x_val, x_test, y_train, y_val, y_test

# sample_X and sample_Y should be of the same size
X = sample_X
print(X.shape)

Y = sample_Y
print(Y.shape)

"""### Data processing on small dataset"""

# With imputation
X, num_static_cols = process_data(X,Y,mins_series,maxes_series,avg_dict,static)

final_X_columns = np.array(X.columns)
print("final X columns: ", final_X_columns)

# With make_discrete (to be checked later)
# X_discrete, num_static_cols_discrete = process_data(X, Y, static,make_discrete=True)
# final_X_discrete_columns = np.array(X_discrete.columns)
# print("final X_discrete columns: ", final_X_discrete_columns)

#lengths_df = lengths_df[lengths_df['subject_id'].isin([3,4,17,20,21,23,25])]
#print(lengths_df)
print(X.columns)
x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test = stratified_split(X, Y, sample_Y_for_stratification, static, lengths_df)

print("Stratified split results:\n")

print("Train:")
print('x_train size: ',x_train.shape)
print("y_train size: ", y_train.shape)
print("lengths_train size: ", lengths_train.shape,"\n")

print("Validation:")
print('x_val size: ', x_val.shape)
print("y_val size: ", y_val.shape)
print("lengths_val size: ", lengths_train.shape,"\n")

print("Test:")
print('x_test size: ', x_test.shape)
print("y_test size: ", y_test.shape)
print("lengths_test size: ", lengths_test.shape,"\n")

# Check
print("Is the sum of train + val + test the same as the size of X?")
print(x_train.shape[0] + x_val.shape[0] + x_test.shape[0] == X.shape[0])

print("Check column names:\n")
print('x_train columns: ',x_train.columns, "\n")
#print(x_train.head())
print("y_train columns: ", y_train.columns, "\n")
#print(y_train.head())
print("lengths_train columns: ", lengths_train.columns, "\n")
#print(y_train.head())

# 3D tensors

# Train
print("Train - 3D tensor conversion starting...")
X_tensor_train,Y_tensor_train,lengths_tensor_train = make_3d_tensors(x_train, y_train, lengths_train)

print("Train - Final dimensions - After conversion to 3D tensors:")
print("Train - Final shape X tensor",np.shape(X_tensor_train))
print("Train - Final shape Y tensor",np.shape(Y_tensor_train))
print("Train - Final shape lengths tensor", np.shape(lengths_tensor_train), "\n")

# Validation
print("Validation - 3D tensor conversion starting...")
X_tensor_val,Y_tensor_val,lengths_tensor_val = make_3d_tensors(x_val, y_val, lengths_val)

print("Validation - Final dimensions - After conversion to 3D tensors:")
print("Validation - Final shape X tensor",np.shape(X_tensor_val))
print("Validation - Final shape Y tensor",np.shape(Y_tensor_val))
print("Validation - Final shape lengths tensor", np.shape(lengths_tensor_val), "\n")

# Test 
print("Test - 3D tensor conversion starting...")
X_tensor_test,Y_tensor_test,lengths_tensor_test = make_3d_tensors(x_test, y_test, lengths_test)

print("Test - Final dimensions - After conversion to 3D tensors:")
print("Test - Final shape X tensor",np.shape(X_tensor_test))
print("Test - Final shape Y tensor",np.shape(Y_tensor_test))
print("Test - Final shape lengths tensor", np.shape(lengths_tensor_test), "\n")

# 3D tensors with slices of 6 hours each

#X_tensor_slices_train, Y_tensor_slices_train, gap_tensor_slices_train = make_3d_tensor_slices(X_tensor_train, Y_tensor_train, lengths_tensor_train, slice_size=6, gap_time=6,return_gap=True)
X_tensor_slices_train, Y_tensor_slices_train = make_3d_tensor_slices(X_tensor_train, Y_tensor_train, lengths_tensor_train, slice_size=6, gap_time=6,return_gap=False)

#X_tensor_slices_val, Y_tensor_slices_val, gap_tensor_slices_val = make_3d_tensor_slices(X_tensor_val, Y_tensor_val, lengths_tensor_val, slice_size=6, gap_time=6,return_gap=True)
X_tensor_slices_val, Y_tensor_slices_val = make_3d_tensor_slices(X_tensor_val, Y_tensor_val, lengths_tensor_val, slice_size=6, gap_time=6,return_gap=False)

#X_tensor_slices_test, Y_tensor_slices_test, gap_tensor_slices_test = make_3d_tensor_slices(X_tensor_test, Y_tensor_test, lengths_tensor_test, slice_size=6, gap_time=6,return_gap=True)
X_tensor_slices_test, Y_tensor_slices_test= make_3d_tensor_slices(X_tensor_test, Y_tensor_test, lengths_tensor_test, slice_size=6, gap_time=6,return_gap=False)

# Need to reshape before passing on to make_3d_tensor_slices
#print(type(X_tensor))

print("Train - Final dimensions - After conversion to 3D tensors with slices:")
print(X_tensor_slices_train.shape)
print(Y_tensor_slices_train.shape)
#print(gap_tensor_slices_train.shape, "\n")

print("Validation - Final dimensions - After conversion to 3D tensors with slices:")
print(X_tensor_slices_val.shape)
print(Y_tensor_slices_val.shape)
#print(gap_tensor_slices_val.shape, "\n")

print("Test - Final dimensions - After conversion to 3D tensors with slices:")
print(X_tensor_slices_test.shape)
print(Y_tensor_slices_test.shape)
#print(gap_tensor_slices_test.shape)

#who_ls
del_list = ['X','X_tensor_test', 'X_tensor_train', 'X_tensor_val', 
            'Y', 'Y_tensor_test', 'Y_tensor_train', 'Y_tensor_val',
            'final_X_columns', 'indices', 'lengths', 'lengths_add', 
            'lengths_df', 'lengths_tensor_test', 'lengths_tensor_train',
            'lengths_tensor_val', 'lengths_test', 'lengths_train',
            'lengths_val', 'max_len', 'num_static_cols', 'sample_X','sample_Y',
            'sample_Y_for_stratification', 'static', 'x_test', 'x_train', 
            'x_val', 'y_test', 'y_train', 'y_val']

# Delete variables that are not needed anymore
del X
del X_tensor_test
del X_tensor_train
del X_tensor_val
del Y
del Y_tensor_test
del Y_tensor_train
del Y_tensor_val
del final_X_columns
del lengths
del lengths_add
del lengths_df
del lengths_tensor_test
del lengths_tensor_train
del lengths_val
del num_static_cols
del sample_X
del sample_Y
del sample_Y_for_stratification
del static
del x_test
del x_train
del x_val
del y_test
del y_train
del y_val

# Check that everything except sliced tensors and methods is deleted
#who_ls

# Start filling out final sliced tensors (to which we will concatenate the next ones)
X_tensor_slices_train_final = X_tensor_slices_train
X_tensor_slices_val_final = X_tensor_slices_val
X_tensor_slices_test_final = X_tensor_slices_test

Y_tensor_slices_train_final = Y_tensor_slices_train
Y_tensor_slices_val_final = Y_tensor_slices_val
Y_tensor_slices_test_final = Y_tensor_slices_test

print(X_tensor_slices_train_final.shape)
print(X_tensor_slices_val_final.shape)
print(X_tensor_slices_test_final.shape)

print(Y_tensor_slices_train_final.shape)
print(Y_tensor_slices_val_final.shape)
print(Y_tensor_slices_test_final.shape)

print(X_tensor_slices_train.shape)
print(X_tensor_slices_val.shape)
print(X_tensor_slices_test.shape)

print(Y_tensor_slices_train.shape)
print(Y_tensor_slices_val.shape)
print(Y_tensor_slices_test.shape)

"""### Finalized queries"""

query_X = '''select distinct *
from (
select t.subject_id,
t.hadm_id,
t.icustay_id,
t.hr,

-- Labs
ALB.ALBUMIN_avgval,
ALT.ALT_avgval,
AMYLASE.AMYLASE_avgval,
ANION_GAP.ANION_GAP_avgval,
AST.AST_avgval,
BANDS.BANDS_avgval,
BICARBONATE.BICARBONATE_avgval,
BILIRUBIN.BILIRUBIN_avgval,
BUN.BUN_avgval,
CHLORIDE.CHLORIDE_avgval,
CK.CK_avgval,
CKISO.CKISO_avgval,
CREATININE.CREATININE_avgval,
CRP.CRP_avgval,
D_DIMER.D_DIMER_avgval,
FIBRINOGEN.FIBRINOGEN_avgval,
FREE_CALCIUM.FREE_CALCIUM_avgval,
GLUCOSE.GLUCOSE_avgval,
HEMATOCRIT.HEMATOCRIT_avgval,
HEMOGLOBIN.HEMOGLOBIN_avgval,
INR.INR_avgval,
LACTATE.LACTATE_avgval,
LIPASE.LIPASE_avgval,
NEUTROPHILS.NEUTROPHILS_avgval,
NTPROBNP.NTPROBNP_avgval,
PH.PH_avgval,
PLATELET.PLATELET_avgval,
POTASSIUM.POTASSIUM_avgval,
--PT.PT_avgval,
PTT.PTT_avgval,
RBC.RBC_avgval,
SODIUM.SODIUM_avgval,
TROPONIN_T.TROPONIN_T_avgval,
WBC.WBC_avgval,

-- VITALS
HR.HR_avgval,
DiasBP.DiasBP_avgval,
SysBP.SysBP_avgval,
MeanBP.MeanBP_avgval,
RespRate.RespRate_avgval,
TempC.TempC_avgval,
CRYSTALLOID.CRYSTALLOID_binary,
CRYSTALLOID.CRYSTALLOID_sum,
VASOPRESSORS.VASOPRESSOR_binary
from `team_M.to_be_merged` t

left join `team_M.ALB_join`as ALB on t.subject_id = ALB.subject_id and t.hadm_id = ALB.hadm_id and t.icustay_id = ALB.icustay_id and t.hr = ALB.hr
left join `team_M.ALT_join`as ALT on t.subject_id = ALT.subject_id and t.hadm_id = ALT.hadm_id and t.icustay_id = ALT.icustay_id and t.hr = ALT.hr
left join `team_M.AMYLASE_join`as AMYLASE on t.subject_id = AMYLASE.subject_id and t.hadm_id = AMYLASE.hadm_id and t.icustay_id = AMYLASE.icustay_id and t.hr = AMYLASE.hr
left join `team_M.ANION_GAP_join`as ANION_GAP on t.subject_id = ANION_GAP.subject_id and t.hadm_id = ANION_GAP.hadm_id and t.icustay_id = ANION_GAP.icustay_id and t.hr = ANION_GAP.hr
left join `team_M.AST_join`as AST on t.subject_id = AST.subject_id and t.hadm_id = AST.hadm_id and t.icustay_id = AST.icustay_id and t.hr = AST.hr
left join `team_M.BANDS_join`as BANDS on t.subject_id = BANDS.subject_id and t.hadm_id = BANDS.hadm_id and t.icustay_id = BANDS.icustay_id and t.hr = BANDS.hr
left join `team_M.BICARBONATE_join`as BICARBONATE on t.subject_id = BICARBONATE.subject_id and t.hadm_id = BICARBONATE.hadm_id and t.icustay_id = BICARBONATE.icustay_id and t.hr = BICARBONATE.hr
left join `team_M.BILIRUBIN_join`as BILIRUBIN on t.subject_id = BILIRUBIN.subject_id and t.hadm_id = BILIRUBIN.hadm_id and t.icustay_id = BILIRUBIN.icustay_id and t.hr = BILIRUBIN.hr
left join `team_M.BUN_join`as BUN on t.subject_id = BUN.subject_id and t.hadm_id = BUN.hadm_id and t.icustay_id = BUN.icustay_id and t.hr = BUN.hr
left join `team_M.CHLORIDE_join`as CHLORIDE on t.subject_id = CHLORIDE.subject_id and t.hadm_id = CHLORIDE.hadm_id and t.icustay_id = CHLORIDE.icustay_id and t.hr = CHLORIDE.hr
left join `team_M.CK_join`as CK on t.subject_id = CK.subject_id and t.hadm_id = CK.hadm_id and t.icustay_id = CK.icustay_id and t.hr = CK.hr
left join `team_M.CKISO_join`as CKISO on t.subject_id = CKISO.subject_id and t.hadm_id = CKISO.hadm_id and t.icustay_id = CKISO.icustay_id and t.hr = CKISO.hr
left join `team_M.CREATININE_join`as CREATININE on t.subject_id = CREATININE.subject_id and t.hadm_id = CREATININE.hadm_id and t.icustay_id = CREATININE.icustay_id and t.hr = CREATININE.hr
left join `team_M.CRP_join`as CRP on t.subject_id = CRP.subject_id and t.hadm_id = CRP.hadm_id and t.icustay_id = CRP.icustay_id and t.hr = CRP.hr
left join `team_M.D_DIMER_join`as D_DIMER on t.subject_id = D_DIMER.subject_id and t.hadm_id = D_DIMER.hadm_id and t.icustay_id = D_DIMER.icustay_id and t.hr = D_DIMER.hr
left join `team_M.FIBRINOGEN_join`as FIBRINOGEN on t.subject_id = FIBRINOGEN.subject_id and t.hadm_id = FIBRINOGEN.hadm_id and t.icustay_id = FIBRINOGEN.icustay_id and t.hr = FIBRINOGEN.hr
left join `team_M.FREE_CALCIUM_join`as FREE_CALCIUM on t.subject_id = FREE_CALCIUM.subject_id and t.hadm_id = FREE_CALCIUM.hadm_id and t.icustay_id = FREE_CALCIUM.icustay_id and t.hr = FREE_CALCIUM.hr
left join `team_M.GLUCOSE_join` as GLUCOSE on t.subject_id = GLUCOSE.subject_id and t.hadm_id = GLUCOSE.hadm_id and t.icustay_id = GLUCOSE.icustay_id and t.hr = GLUCOSE.hr
left join `team_M.HEMATOCRIT_join`as HEMATOCRIT on t.subject_id = HEMATOCRIT.subject_id and t.hadm_id = HEMATOCRIT.hadm_id and t.icustay_id = HEMATOCRIT.icustay_id and t.hr = HEMATOCRIT.hr
left join `team_M.HEMOGLOBIN_join`as HEMOGLOBIN on t.subject_id = HEMOGLOBIN.subject_id and t.hadm_id = HEMOGLOBIN.hadm_id and t.icustay_id = HEMOGLOBIN.icustay_id and t.hr = HEMOGLOBIN.hr
left join `team_M.INR_join`as INR on t.subject_id = INR.subject_id and t.hadm_id = INR.hadm_id and t.icustay_id = INR.icustay_id and t.hr = INR.hr
left join `team_M.LACTATE_join`as LACTATE on t.subject_id = LACTATE.subject_id and t.hadm_id = LACTATE.hadm_id and t.icustay_id = LACTATE.icustay_id and t.hr = LACTATE.hr
left join `team_M.LIPASE_join`as LIPASE on t.subject_id = LIPASE.subject_id and t.hadm_id = LIPASE.hadm_id and t.icustay_id = LIPASE.icustay_id and t.hr = LIPASE.hr
left join `team_M.NEUTROPHILS_join`as NEUTROPHILS on t.subject_id = NEUTROPHILS.subject_id and t.hadm_id = NEUTROPHILS.hadm_id and t.icustay_id = NEUTROPHILS.icustay_id and t.hr = NEUTROPHILS.hr
left join `team_M.NTPROBNP_join`as NTPROBNP on t.subject_id = NTPROBNP.subject_id and t.hadm_id = NTPROBNP.hadm_id and t.icustay_id = NTPROBNP.icustay_id and t.hr = NTPROBNP.hr
left join `team_M.PH_join` as PH on t.subject_id = PH.subject_id and t.hadm_id = PH.hadm_id and t.icustay_id = PH.icustay_id and t.hr = PH.hr
left join `team_M.PLATELET_join` as PLATELET on t.subject_id = PLATELET.subject_id and t.hadm_id = PLATELET.hadm_id and t.icustay_id = PLATELET.icustay_id and t.hr = PLATELET.hr
left join `team_M.POTASSIUM_join` as POTASSIUM on t.subject_id = POTASSIUM.subject_id and t.hadm_id = POTASSIUM.hadm_id and t.icustay_id = POTASSIUM.icustay_id and t.hr = POTASSIUM.hr
--left join `team_M.PT_join`as PT on t.subject_id = PT.subject_id and t.hadm_id = PT.hadm_id and t.icustay_id = PT.icustay_id and t.hr = PT.hr
left join `team_M.PTT_join`as PTT on t.subject_id = PTT.subject_id and t.hadm_id = PTT.hadm_id and t.icustay_id = PTT.icustay_id and t.hr = PTT.hr
left join `team_M.RBC_join`as RBC on t.subject_id = RBC.subject_id and t.hadm_id = RBC.hadm_id and t.icustay_id = RBC.icustay_id and t.hr = RBC.hr
left join `team_M.SODIUM_join`as SODIUM on t.subject_id = SODIUM.subject_id and t.hadm_id = SODIUM.hadm_id and t.icustay_id = SODIUM.icustay_id and t.hr = SODIUM.hr
left join `team_M.TROPONIN_T_join`as TROPONIN_T on t.subject_id = TROPONIN_T.subject_id and t.hadm_id = TROPONIN_T.hadm_id and t.icustay_id = TROPONIN_T.icustay_id and t.hr = TROPONIN_T.hr
left join `team_M.WBC_join`as WBC on t.subject_id = WBC.subject_id and t.hadm_id = WBC.hadm_id and t.icustay_id = WBC.icustay_id and t.hr = WBC.hr

-- Vital signs
left join `team_M.HR_join`as HR on t.subject_id = HR.subject_id and t.hadm_id = HR.hadm_id and t.icustay_id = HR.icustay_id and t.hr = HR.hr
left join `team_M.DiasBP_join`as DiasBP on t.subject_id = DiasBP.subject_id and t.hadm_id = DiasBP.hadm_id and t.icustay_id = DiasBP.icustay_id and t.hr = DiasBP.hr
left join `team_M.SysBP_join`as SysBP on t.subject_id = SysBP.subject_id and t.hadm_id = SysBP.hadm_id and t.icustay_id = SysBP.icustay_id and t.hr = SysBP.hr
left join `team_M.MeanBP_join`as MeanBP on t.subject_id = MeanBP.subject_id and t.hadm_id = MeanBP.hadm_id and t.icustay_id = MeanBP.icustay_id and t.hr = MeanBP.hr
left join `team_M.RespRate_join`as RespRate on t.subject_id = RespRate.subject_id and t.hadm_id = RespRate.hadm_id and t.icustay_id = RespRate.icustay_id and t.hr = RespRate.hr
left join `team_M.TempC_join`as TempC on t.subject_id = TempC.subject_id and t.hadm_id = TempC.hadm_id and t.icustay_id = TempC.icustay_id and t.hr = TempC.hr

-- vasopressors
left join `team_M.VASOPRESSORS_join_reviewed_new_2`as VASOPRESSORS on t.subject_id = VASOPRESSORS.subject_id and t.hadm_id = VASOPRESSORS.hadm_id and t.icustay_id = VASOPRESSORS.icustay_id and t.hr = VASOPRESSORS.hr

-- crystalloids
left join `team_M.CRYSTALLOID_join_reviewed_new_2`as CRYSTALLOID on t.subject_id = CRYSTALLOID.subject_id and t.hadm_id = CRYSTALLOID.hadm_id and t.icustay_id = CRYSTALLOID.icustay_id and t.hr = CRYSTALLOID.hr

WHERE t.subject_id > ? and t.subject_id <= ?

ORDER BY subject_id, hadm_id, icustay_id, hr)
ORDER BY subject_id, hadm_id, icustay_id, hr'''

query_Y = '''
select t.*,
if(y.PF_minval_less is null, 0, y.PF_minval_less) as Y
from  `team_M.to_be_merged` t
left join  `team_M.PaO2_FiO2_ratios_per_hour_with_booleans` y
on t.subject_id = y.subject_id and t.hadm_id = y.hadm_id and t.icustay_id = y.icustay_id and t.hr = y.hr
where t.subject_id > ? and t.subject_id <= ?'''

query_Y_for_stratification = '''select t.subject_id,
t.hadm_id,
t.icustay_id,
if(y.max_PF is null, 0, y.max_PF) as Y
from `team_M.to_be_merged` t 
left join (select subject_id,
hadm_id,
icustay_id,
max(PF_minval_less) as max_PF
from `team_M.PaO2_FiO2_ratios_per_hour_with_booleans`
group by subject_id, hadm_id, icustay_id
order by subject_id, hadm_id, icustay_id) y
on t.subject_id = y.subject_id and t.hadm_id = y.hadm_id and t.icustay_id = y.icustay_id
where t.subject_id > ? and t.subject_id <= ?
group by subject_id, hadm_id, icustay_id, max_PF
order by subject_id, hadm_id, icustay_id'''

query_static = '''
select subject_id,
hadm_id,
icustay_id,
intime,
outtime,
hospital_expire_flag,
gender,
age_category,
ethnicity_category,
admission_type,
admission_location_category,
insurance_category,
marital_status_category
from `team_M.demographics2_final_new`
where subject_id > ? and subject_id <= ?
order by subject_id, hadm_id,icustay_id'''

query_lengths = '''
select distinct subject_id, 
         hadm_id,
         icustay_id,
         if(floor(los*24)+1 > 168, 168, floor(los*24)+1)  as los 
 from `team_M.LuStep1_new_new`
 where icu_stay_rank = 1 and subject_id > ? and subject_id <= ?

 group by subject_id, hadm_id, icustay_id, los
 order by subject_id, hadm_id, icustay_id;'''

def sample_query(query_params, query):
  job_config = bigquery.QueryJobConfig()
  job_config.query_parameters = query_params
  
  query_job = client.query(query, location ='US',job_config=job_config)
  sample = query_job.to_dataframe()
  
  return(sample)

def load_and_process(sample_X,sample_Y,minima,maxima,averages,static):
  load_data(sample_X, sample_Y, static)
  # Process data
  X, num_static_cols = process_data(sample_X,sample_Y,minima,maxima,averages,static)
  final_X_columns = np.array(X.columns)
  print("final X columns: ", final_X_columns)
  return(X, num_static_cols)
  
def strat_split(X,sample_Y,sample_Y_for_stratification, static,lengths_df):
  x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test = stratified_split(X, sample_Y, sample_Y_for_stratification, static, lengths_df)
  
  print("Stratified split results:\n")
  print("Train:")
  print('x_train size: ',x_train.shape)
  print("y_train size: ", y_train.shape)
  print("lengths_train size: ", lengths_train.shape,"\n")
  
  print("Validation:")
  print('x_val size: ', x_val.shape)
  print("y_val size: ", y_val.shape)
  print("lengths_val size: ", lengths_train.shape,"\n")
  
  print("Test:")
  print('x_test size: ', x_test.shape)
  print("y_test size: ", y_test.shape)
  print("lengths_test size: ", lengths_test.shape,"\n")
  
  # Check
  print("Is the sum of train + val + test the same as the size of X?")
  print(x_train.shape[0] + x_val.shape[0] + x_test.shape[0] == X.shape[0])
  
  return(x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test)
  
def threeD_tens(x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test):
  print("Train - 3D tensor conversion starting...")
  X_tensor_train,Y_tensor_train,lengths_tensor_train = make_3d_tensors(x_train, y_train, lengths_train)
  
  print("Train - Final dimensions - After conversion to 3D tensors:")
  print("Train - Final shape X tensor",np.shape(X_tensor_train))
  print("Train - Final shape Y tensor",np.shape(Y_tensor_train))
  print("Train - Final shape lengths tensor", np.shape(lengths_tensor_train), "\n")
  
  # Validation
  print("Validation - 3D tensor conversion starting...")
  X_tensor_val,Y_tensor_val,lengths_tensor_val = make_3d_tensors(x_val, y_val, lengths_val)
  
  print("Validation - Final dimensions - After conversion to 3D tensors:")
  print("Validation - Final shape X tensor",np.shape(X_tensor_val))
  print("Validation - Final shape Y tensor",np.shape(Y_tensor_val))
  print("Validation - Final shape lengths tensor", np.shape(lengths_tensor_val), "\n")

  # Test 
  print("Test - 3D tensor conversion starting...")
  X_tensor_test,Y_tensor_test,lengths_tensor_test = make_3d_tensors(x_test, y_test, lengths_test)
  
  print("Test - Final dimensions - After conversion to 3D tensors:")
  print("Test - Final shape X tensor",np.shape(X_tensor_test))
  print("Test - Final shape Y tensor",np.shape(Y_tensor_test))
  print("Test - Final shape lengths tensor", np.shape(lengths_tensor_test), "\n")
  
  return(X_tensor_train, Y_tensor_train, lengths_tensor_train, X_tensor_val, Y_tensor_val, lengths_tensor_val, X_tensor_test, Y_tensor_test, lengths_tensor_test)
  
def threeD_tens_slices(X_tensor_train, Y_tensor_train, lengths_tensor_train, X_tensor_val, Y_tensor_val, lengths_tensor_val, X_tensor_test, Y_tensor_test, lengths_tensor_test):
  X_tensor_slices_train, Y_tensor_slices_train= make_3d_tensor_slices(X_tensor_train, Y_tensor_train, lengths_tensor_train, slice_size=6, gap_time=6,return_gap=False)
  X_tensor_slices_val, Y_tensor_slices_val = make_3d_tensor_slices(X_tensor_val, Y_tensor_val, lengths_tensor_val, slice_size=6, gap_time=6,return_gap=False)
  X_tensor_slices_test, Y_tensor_slices_test = make_3d_tensor_slices(X_tensor_test, Y_tensor_test, lengths_tensor_test, slice_size=6, gap_time=6,return_gap=False)
  
  print("Train - Final dimensions - After conversion to 3D tensors with slices:")
  print(X_tensor_slices_train.shape)
  print(Y_tensor_slices_train.shape)
  
  print("Validation - Final dimensions - After conversion to 3D tensors with slices:")
  print(X_tensor_slices_val.shape)
  print(Y_tensor_slices_val.shape)
  
  print("Test - Final dimensions - After conversion to 3D tensors with slices:")
  print(X_tensor_slices_test.shape)
  print(Y_tensor_slices_test.shape)
  
  return(X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test)

def run_chunk(chunk,minima,maxima,averages):
  min_idx = chunk
  max_idx = chunk + 1000
  
  print("Min subject id index: >= ", min_idx)
  print("Max subject id index: < ", max_idx)
  
  query_params = [
    bigquery.ScalarQueryParameter(None, 'INT64', min_idx),
    bigquery.ScalarQueryParameter(None, 'INT64', max_idx)
  ]
  
  sample_X = sample_query(query_params,query_X)
  sample_Y = sample_query(query_params,query_Y)
  sample_Y_for_stratification = sample_query(query_params,query_Y_for_stratification)
  static = sample_query(query_params,query_static)
  lengths_df = sample_query(query_params,query_lengths)
  
  # Adding index column (just in case)
  #lengths_add = pd.DataFrame(list(range(0,lengths_df.shape[0])))
  #lengths_df[['index']] = lengths_add
  
  X, num_static_columns = load_and_process(sample_X,sample_Y,minima,maxima,averages,static)
  
  x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test = strat_split(X, sample_Y, sample_Y_for_stratification, static, lengths_df)
  
  X_tensor_train, Y_tensor_train, lengths_tensor_train, X_tensor_val, Y_tensor_val, lengths_tensor_val, X_tensor_test, Y_tensor_test, lengths_tensor_test = threeD_tens(x_train, x_val, x_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test)
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = threeD_tens_slices(X_tensor_train, Y_tensor_train, lengths_tensor_train, X_tensor_val, Y_tensor_val, lengths_tensor_val, X_tensor_test, Y_tensor_test, lengths_tensor_test)
  
  del X
  del X_tensor_test
  del X_tensor_train
  del X_tensor_val
  del Y_tensor_test
  del Y_tensor_train
  del Y_tensor_val
  del lengths_df
  del lengths_tensor_test
  del lengths_tensor_train
  del lengths_val
  del num_static_columns
  del sample_X
  del sample_Y
  del sample_Y_for_stratification
  del static
  del x_test
  del x_train
  del x_val
  del y_test
  del y_train
  del y_val
  
  return(X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test)

# Feature list - Check

Features = ['subject_id' ,'hadm_id' ,'icustay_id', 'hr', 'absolute_time' ,'albumin_avgval',
 'alt_avgval', 'amylase_avgval', 'anion_gap_avgval' ,'ast_avgval',
 'bands_avgval', 'bicarbonate_avgval', 'bilirubin_avgval', 'bun_avgval',
 'chloride_avgval', 'ck_avgval' ,'ckiso_avgval', 'creatinine_avgval',
 'crp_avgval' ,'d_dimer_avgval' ,'fibrinogen_avgval', 'free_calcium_avgval',
 'glucose_avgval', 'hematocrit_avgval', 'hemoglobin_avgval' ,'inr_avgval',
 'lactate_avgval', 'lipase_avgval' ,'neutrophils_avgval' ,'ntprobnp_avgval',
 'ph_avgval' ,'platelet_avgval', 'potassium_avgval', 'ptt_avgval',
 'rbc_avgval', 'sodium_avgval' ,'troponin_t_avgval', 'wbc_avgval', 'hr_avgval',
 'diasbp_avgval', 'sysbp_avgval' ,'meanbp_avgval' ,'resprate_avgval',
 'tempc_avgval' ,'crystalloid_binary' ,'crystalloid_sum',
 'vasopressor_binary', 'age_category' ,'gender_F','gender_M',
 'admission_type_ELECTIVE', 'admission_type_EMERGENCY',
 'admission_type_URGENT' ,'admission_location_category_CLINIC_REFERRAL',
 'admission_location_category_EMERGENCY_ROOM',
 'admission_location_category_HOSP_TRANSFER',
 'admission_location_category_OTHER',
 'admission_location_category_PHYS_REFERRAL',
 'marital_status_category_DIVORCED', 'marital_status_category_MARRIED',
 'marital_status_category_OTHER', 'marital_status_category_SEPARATED',
 'marital_status_category_SINGLE' ,'marital_status_category_WIDOWED',
 'ethnicity_category_AMERICAN INDIAN', 'ethnicity_category_ASIAN',
 'ethnicity_category_BLACK'  ,'ethnicity_category_HISPANIC/LATINO',
 'ethnicity_category_OTHER' ,'ethnicity_category_WHITE']

print(len(Features))

"""### There is no patient ID >= 32811 AND <= 39999

> Indented block
"""

import pickle

result_dict = {}

# Iter 1
for chunk in range(0,33000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val, 'X_test': X_tensor_slices_test, 'Y_train': Y_tensor_slices_train, 'Y_val': Y_tensor_slices_val, 'Y_test': Y_tensor_slices_test}
      
  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

for chunk in range(40000,51000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  #result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}
  
  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

print(result_dict)

for chunk in range(40000,51000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

import pickle
result_dict = {}

for chunk in range(51000,61000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

for chunk in range(61000,71000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)  
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

print(result_dict)

for chunk in range(71000,81000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

print(result_dict)

for chunk in range(81000,90000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()

print(result_dict)

for chunk in range(91000,100000,1000):
  
  X_tensor_slices_train, Y_tensor_slices_train, X_tensor_slices_val, Y_tensor_slices_val, X_tensor_slices_test, Y_tensor_slices_test = run_chunk(chunk, mins_series, maxes_series, avg_dict)
  result_dict[str(chunk)] = {'X_train': X_tensor_slices_train.shape, 'X_val': X_tensor_slices_val.shape, 'X_test': X_tensor_slices_test.shape, 'Y_train': Y_tensor_slices_train.shape, 'Y_val': Y_tensor_slices_val.shape, 'Y_test': Y_tensor_slices_test.shape}

  str_chunk = str(chunk)
  
  # Train
  output_X_tensor_train = open('/content/drive/My Drive/X_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_train, output_X_tensor_train)
  output_X_tensor_train.close()

  output_Y_tensor_train = open('/content/drive/My Drive/Y_tensor_train_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_train, output_Y_tensor_train)
  output_Y_tensor_train.close()

  # Val
  output_X_tensor_val = open('/content/drive/My Drive/X_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_val, output_X_tensor_val)
  output_X_tensor_val.close()

  output_Y_tensor_val = open('/content/drive/My Drive/Y_tensor_val_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_val, output_Y_tensor_val)
  output_Y_tensor_val.close()

  # Test
  output_X_tensor_test = open('/content/drive/My Drive/X_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(X_tensor_slices_test, output_X_tensor_test)
  output_X_tensor_test.close()

  output_Y_tensor_test = open('/content/drive/My Drive/Y_tensor_test_chunk_'+str_chunk+'.pkl', 'wb')
  pickle.dump(Y_tensor_slices_test, output_Y_tensor_test)
  output_Y_tensor_test.close()